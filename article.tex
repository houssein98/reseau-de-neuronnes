\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel}
\usepackage{numprint}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}

%opening
\title{Apprentissage par des réseaux de neurones}
\author{E. Beghdadi, M. Marrakchi Benazzouz, T. Chabal \\ Y. Vanlaer, T. Giraudon \\  \\ sous la direction de Michel Terré}

\begin{document}

\maketitle

\begin{abstract}
Ce projet vise à coder un réseau de neurones capable de classer des données variés.

\end{abstract}

\section{Introduction}
En 1963, l'Américain Donald Michie créé Menace, \textit{Machine Educable Noughts And Crosses Engine},
une machine capable de rivaliser avec des joueurs humains au jeu du Tic-Tac-Toe, plus connu en France sous le nom de morpion.
Pour réaliser cette prouesse technologique, il utilise alors la technique de \textit{l'apprentissage par renforcement}.
Celle-ci consiste à faire jouer la machine un grand nombre de fois contre un joueur réel et à apprendre de ces parties
en corrigeant sa stratégie au fur et à mesure. Cette réussite signe alors la naissance du machine learning tel que
nous le connaissons aujourd'hui. \\
Dans les années 1970-1980, beaucoup de progrès théoriques sont réalisés : on imagine ainsi un système
directement inspiré du vivant, le réseau de neurones. Celui-ci consiste en un agencement de couches de neurones successives,
appelés les \textit{layers}, indexés par $L$. Plusieurs types de liaisons entre les neurones sont possibles; dans notre cas, chaque neurone du
\textit{layer} $L$ est relié à l'intégralité des neurones des \textit{layers} $L-1$ et $L+1$. A la liaison entre
le $i$-ème neurone du \textit{layer} $L-1$ et le $j$-ème du \textit{layer} $L$ correspond un poids $w^L_{ij}$. Les premier et dernier \textit{layers}
jouent un rôle particulier : le premier correspond aux \textit{inputs}, les données d'entrée, et le dernier aux \textit{outputs},
les données de sortie. Les autres \textit{layers} sont qualifiés d'\textit{hidden layers}.
\\

\begin{figure}[H]
\centering
\includegraphics[scale = 0.4]{layers}
\caption{Structure du réseau de neurones}
 
\end{figure}

Le neurone lui-même est composé d'une porte d'entrée qui réalise la combinaison linéaire des sorties des neurones de la couche
précédente qui lui sont connectés, pondérés par les poids associés aux liaisons concernées. Un \textit{bias} peut être ajouté
à la combinaison linéaire. La valeur obtenue
passe alors dans une fonction seuil. Ici on utilise la sigmoïde $$ \varphi : x \longmapsto \frac{1}{1+\exp(-x)}$$ qui a l'avantage
d'être continûment dérivable. La valeur de sortie du neurone est la valeur qu'on obtient alors.

\begin{figure}[H]
\centering
\includegraphics[scale = 0.6]{neuron}
\caption{Neurone}
 
\end{figure}

L'apprentissage se réalise de la façon suivante : on rentre des \textit{inputs} dont on connait les \textit{outputs} et on minimise
l'erreur, définie comme le carré de la différence entre le vecteur de sortie et celui des \textit{outputs} prévus, et ceci par rapport
aux poids du réseau. Pour ce faire, on utilise un algorithme de descente du gradient, dont le pas et le nombre d'itérations 
sont des paramètres. On utilise le même procédé pour les \textit{bias}. On opère ainsi pour l'ensemble des données du \textit{dataset}, 
dans le désordre et plusieurs fois. Une fois que l'erreur est inférieure à un seuil choisi, le réseau est prêt à être utilisé 
sur des \textit{inputs} inconnus.
\\
\\
Bien que cet algorithme ait été élaboré dans les années 80, il ne pouvait pas vraiment être utilisé efficacement à l'époque à cause 
du manque de données sur lesquelles s'entraîner. C'est lors de ces dernières années qu'il a connu un renouveau considérable grâce
à l'émergence du \textit{big data}, un ensemble de technologies permettant la collecte et le traitement efficace d'un très grand nombre
de données. Les réseaux de neurones sont donc un sujet majeur du monde actuel, capables de donner des informations précises sur des systèmes variés.
\\
\\
Dans ce projet, nous avons tout d'abord tenté de réaliser un réseau de neurones capable de fonctionner sur des exemples simples, comme 
la somme de deux nombres binaires petits ou la classification d'espèces d'iris à partir d'un \textit{dataset} trouvé
sur Internet. Ensuite, nous avons essayé d'adapter nos réseaux à une problématique plus complexe, la classification de chromosomes
à partir d'images.


\section{Résultats}

\subsection{Classification d'iris}

Nous avons utilisé un \textit{dataset} assez plébiscité par les débutants en \textit{machine learning}. Celui-ci contient $4$ 
informations : la longueur et la largeur du sépale, ains que celles de la pétale. Ces données sont à chaque fois accompagnées 
de l'espèce correspondante, soit l'\textit{iris setosa}, l'\textit{iris versicolor} ou l'\textit{iris virginica}.

Ici, la difficulté vient du réglage des paramètres. En effet, plusieurs variables rentrent en compte :
\begin{itemize}
 \item le nombre de \textit{hidden layers}
 \item la taille de chacun d'entre eux
 \item le pas dans l'algorithme de descente du gradient
 \item le nombre d'itérations dans celui-ci
 \item la précision demandée sur l'erreur finale
\end{itemize}

Par exemple, si le nombre de neurones est trop petit, on cours le risque de rencontrer le problème d'\textit{underfitting}, c'est à dire
que le réseau n'est pas assez riche pour apprendre des données. Au contraire, s'il y a trop de neurones, il peut 
se passer le problème inverse, celui de l'\textit{overfitting}, où le réseau apprend ``par cœur'' le \textit{dataset} et est inefficace 
sur des données non contenues dans celui-ci. \\
Après la détermination des paramètres permettant de minimiser l'erreur en un temps de calcul raisonnable, nous avons réussi à obtenir un 
réseau performant. Voici le profil de l'erreur :

\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[scale = 0.6]{erreur_iris}
\caption{Evolution de l'erreur en fonction du nombre d'entraînements sur le \textit{dataset}}
 
\end{figure}

L'erreur converge donc vers une valeur d'environ $5 \% $.

\subsection{Somme de deux nombres binaires}


\section{Discussion}

Pour apprendre de \textit{datasets} simples, c'est à dire de quelques \textit{inputs}, un réseau de neurones classique est suffisant : l'erreur converge
rapidement vers une erreur assez faible. Cette limite est toutefois non nulle, ce qui peut être un problème dans certains domaines. En effet, 
s'il n'est pas forcément très grave de se tromper dans $5 \% $ des cas lorsqu'on classe des iris, il est bien plus grave de se tromper à ce taux lorsqu'on 
cherche des terroristes : dans ce cas, les ``faux positifs'' où l'on arrête des personnes innocentes $5$ fois sur $100$ sont très délicats à gérer.
Ce réseau de neurones ne peut donc pas être utilisé universellement, une réflexion sur la tolérance de l'erreur doit toujours être menée.

\section{Conclusion}

\section{Sources}

https://www.google.com/imgres?imgurl=http%3A%2F%2Fneuralnetworksanddeeplearning.com%2Fimages%2Ftikz11.png&imgrefurl=http%3A%2F%2Fneuralnetworksanddeeplearning.com%2Fchap1.html&docid=knJaSo4wqQHuCM&tbnid=NTVnecn7gEka_M%3A&vet=10ahUKEwjw2ceLj4XbAhWELlAKHWYACcAQMwikASgCMAI..i&w=597&h=324&client=ubuntu&bih=647&biw=1301&q=neural%20network&ved=0ahUKEwjw2ceLj4XbAhWELlAKHWYACcAQMwikASgCMAI&iact=mrc&uact=8
https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.codeproject.com%2FKB%2Fdotnet%2Fpredictor%2Fneuronmodel.jpg&imgrefurl=https%3A%2F%2Fwww.codeproject.com%2FArticles%2F175777%2FFinancial-predictor-via-neural-network&docid=ANecEnYoAqzpQM&tbnid=PKC-AKzH-pbV8M%3A&vet=10ahUKEwjGyd2zjoXbAhVQJFAKHWdJBNYQMwjPASgYMBg..i&w=400&h=303&client=ubuntu&bih=647&biw=1301&q=neural%20network%20neuron&ved=0ahUKEwjGyd2zjoXbAhVQJFAKHWdJBNYQMwjPASgYMBg&iact=mrc&uact=8

\end{document}
