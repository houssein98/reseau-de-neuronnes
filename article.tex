\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[frenchb]{babel}
\usepackage{numprint}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}

%opening
\title{Apprentissage par des réseaux de neurones}
\author{E. Beghdadi, M. Marrakchi Benazzouz, T. Chabal \\ Y. Vanlaer, T. Giraudon \\  \\ sous la direction de Michel Terré}

\begin{document}

\maketitle

\begin{abstract}
Ce projet vise à coder un réseau de neurones capables de classer des chromosomes à partir d'images de ceux-ci.

\end{abstract}

\section{Introduction}
En 1963, l'Américain Donald Michie créé Menace, \textit{Machine Educable Noughts And Crosses Engine},
une machine capable de rivaliser avec des joueurs humains au jeu du Tic-Tac-Toe, plus connu en France sous le nom de morpion.
Pour réaliser cette prouesse technologique, il utilise alors la technique de \textit{l'apprentissage par renforcement}.
Celle-ci consiste à faire jouer la machine un grand nombre de fois contre un joueur réel et à apprendre de ces parties
en corrigeant sa stratégie au fur et à mesure. Cette réussite signe alors la naissance du machine learning tel que
nous le connaissons aujourd'hui. \\
Dans les années 1970-1980, beaucoup de progrès théoriques sont réalisés : on imagine ainsi un système
directement inspiré du vivant, le réseau de neurones. Celui-ci consiste en un agencement de couches de neurones successives,
appelés les \textit{layers}, indexés par $L$. Plusieurs types de liaisons entre les neurones sont possibles; dans notre cas, chaque neurone du
\textit{layer} $L$ est relié à l'intégralité des neurones des \textit{layers} $L-1$ et $L+1$. A la liaison entre
le $i$-ème neurone du \textit{layer} $L-1$ et le $j$-ème du \textit{layer} $L$ correspond un poids $w^L_{ij}$. Les premier et dernier \textit{layers}
jouent un rôle particulier : le premier correspond aux \textit{inputs}, les données d'entrée, et le dernier aux \textit{outputs},
les données de sortie. Les autres \textit{layers} sont qualifiés d'\textit{hidden layers}.
\\

\begin{figure}[H]
\centering
\includegraphics[scale = 0.4]{layers}
\caption{Structure du réseau de neurones}
 
\end{figure}

Le neurone lui-même est composé d'une porte d'entrée qui réalise la combinaison linéaire des sorties des neurones de la couche
précédente qui lui sont connectés, pondérés par les poids associés aux liaisons concernées. Un \textit{bias} peut être ajouté
à la combinaison linéaire. La valeur obtenue
passe alors dans une fonction seuil. Ici on utilise la sigmoïde $$ \varphi : x \longmapsto \frac{1}{1+\exp(-x)}$$ qui a l'avantage
d'être continûment dérivable. La valeur de sortie du neurone est la valeur qu'on obtient alors.

\begin{figure}[H]
\centering
\includegraphics[scale = 0.6]{neuron}
\caption{Neurone}
 
\end{figure}

L'apprentissage se réalise de la façon suivante : on rentre des \textit{inputs} dont on connait les \textit{outputs} et on minimise
l'erreur, définie comme le carré de la différence entre le vecteur de sortie et celui des \textit{outputs} prévus, et ceci par rapport
aux poids du réseau. Pour ce faire, on utilise un algorithme de descente du gradient, dont le pas et le nombre d'itérations 
sont des paramètres. On utilise le même procédé pour les \textit{bias}. On opère ainsi pour l'ensemble des données du \textit{dataset}, 
dans le désordre et plusieurs fois. Une fois que l'erreur est inférieure à un seuil choisi, le réseau est prêt à être utilisé 
sur des \textit{inputs} inconnus.
\\
Dans ce projet, nous avons tout d'abord tenté de réaliser un réseau de neurones capable de fonctionner sur des exemples simples, comme 
par exemple la somme de deux nombres binaires petits ou la classification d'espèces d'iris à partir d'un \textit{dataset} trouvé
sur Internet. Ensuite, nous avons essayé d'utiliser nos réseaux pour classer des chromosomes à partir d'image.


\section{Résultats}

\subsection{Classification d'iris}

Nous avons utilisé un \textit{dataset} assez plébiscité par les débutants en \textit{machine learning}. Celui-ci contient $4$ 
informations : la longueur et la largeur du sépale, ains que celles de la pétale. Ces données sont à chaque fois accompagnées 
de l'espèce correspondante, soit l'\textit{iris setosa}, l'\textit{iris versicolor} ou l'\textit{iris virginica}.

Ici, la difficulté vient du réglage des paramètres. En effet, plusieurs variables rentrent en compte :
\begin{itemize}
 \item le nombre de \textit{hidden layers}
 \item la taille de chacun d'entre eux
 \item le pas dans l'algorithme de descente du gradient
 \item le nombre d'itérations dans celui-ci
 \item la précision demandée sur l'erreur finale
\end{itemize}

Par exemple, si le nombre de neurones est trop petit, on cours le risque de rencontrer le problème d'\textit{underfitting}, c'est à dire
que le réseau n'est pas assez riche pour apprendre des données. Au contraire, s'il y a trop de neurones, il peut 
se passer le problème inverse, celui de l'\textit{overfitting}, où le réseau apprend ``par cœur'' le \textit{dataset} et est inefficace 
sur des données non contenues dans celui-ci. \\
Après la détermination des paramètres permettant de minimiser l'erreur en un temps de calcul raisonnable, nous avons réussi à obtenir un 
réseau performant. Voici le profil de l'erreur :
\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[scale = 0.6]{erreur_iris}
\caption{Evolution de l'erreur en fonction du nombre d'entraînements sur le \textit{dataset}}
 
\end{figure}



\section{Discussion}

\section{Conclusion}

\section{Sources}

https://www.google.com/imgres?imgurl=http%3A%2F%2Fneuralnetworksanddeeplearning.com%2Fimages%2Ftikz11.png&imgrefurl=http%3A%2F%2Fneuralnetworksanddeeplearning.com%2Fchap1.html&docid=knJaSo4wqQHuCM&tbnid=NTVnecn7gEka_M%3A&vet=10ahUKEwjw2ceLj4XbAhWELlAKHWYACcAQMwikASgCMAI..i&w=597&h=324&client=ubuntu&bih=647&biw=1301&q=neural%20network&ved=0ahUKEwjw2ceLj4XbAhWELlAKHWYACcAQMwikASgCMAI&iact=mrc&uact=8
https://www.google.com/imgres?imgurl=https%3A%2F%2Fwww.codeproject.com%2FKB%2Fdotnet%2Fpredictor%2Fneuronmodel.jpg&imgrefurl=https%3A%2F%2Fwww.codeproject.com%2FArticles%2F175777%2FFinancial-predictor-via-neural-network&docid=ANecEnYoAqzpQM&tbnid=PKC-AKzH-pbV8M%3A&vet=10ahUKEwjGyd2zjoXbAhVQJFAKHWdJBNYQMwjPASgYMBg..i&w=400&h=303&client=ubuntu&bih=647&biw=1301&q=neural%20network%20neuron&ved=0ahUKEwjGyd2zjoXbAhVQJFAKHWdJBNYQMwjPASgYMBg&iact=mrc&uact=8

\end{document}

